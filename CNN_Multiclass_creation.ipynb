{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "94ebmAvysVeA"
      },
      "source": [
        "# Importaciones"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QPJiBA0FaWlB",
        "outputId": "702f5593-ca2c-4721-c913-f368a8880e8a"
      },
      "outputs": [],
      "source": [
        "# Importar librerías necesarias\n",
        "import os\n",
        "import glob\n",
        "import random\n",
        "import pandas as pd\n",
        "\n",
        "# Establecer la semilla para reproducibilidad\n",
        "random.seed(42)\n",
        "\n",
        "# Definir la carpeta para guardar los CSV\n",
        "csv_dir = \"./results/csv_files\"\n",
        "os.makedirs(csv_dir, exist_ok=True)  # Crea la carpeta si no existe\n",
        "\n",
        "# Directorio raíz de las imágenes reales\n",
        "# (Carpeta que contiene subcarpetas \"Bacterial\", \"COVID-19\" y \"Normal\")\n",
        "real_data_dir = \"./data/covid-chest-xray\"\n",
        "\n",
        "# Directorio de las imágenes aumentadas\n",
        "# (Carpeta que contiene subcarpetas \"Bacterial\", \"COVID-19\" y \"Normal\")\n",
        "augmented_data_dir = \"./data/data_augmentation\"\n",
        "\n",
        "# Lista de clases según la estructura de tus carpetas\n",
        "clases = [\"Bacterial\", \"COVID-19\", \"Normal\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Bacterial: 48 imágenes reales\n",
            "COVID-19: 342 imágenes reales\n",
            "Normal: 486 imágenes reales\n"
          ]
        }
      ],
      "source": [
        "def obtener_rutas_imagenes(ruta_base, extension=(\"jpg\", \"jpeg\", \"png\", \"tiff\")):\n",
        "    rutas = []\n",
        "    for ext in extension:\n",
        "        rutas.extend(glob.glob(os.path.join(ruta_base, f\"*.{ext}\")))\n",
        "    return rutas\n",
        "\n",
        "\n",
        "# Crear un diccionario con las rutas de imágenes reales para cada clase\n",
        "imagenes_reales = {}\n",
        "for clase in clases:\n",
        "    ruta_clase = os.path.join(real_data_dir, clase)\n",
        "    imagenes_reales[clase] = obtener_rutas_imagenes(ruta_clase)\n",
        "    print(f\"{clase}: {len(imagenes_reales[clase])} imágenes reales\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Bacterial - Total: 48, Train: 33, Val: 7, Test: 8\n",
            "COVID-19 - Total: 342, Train: 239, Val: 51, Test: 52\n",
            "Normal - Total: 486, Train: 340, Val: 72, Test: 74\n"
          ]
        }
      ],
      "source": [
        "# Definir porcentajes\n",
        "porc_train = 0.70\n",
        "porc_val = 0.15\n",
        "porc_test = 0.15\n",
        "\n",
        "# Diccionarios para almacenar las rutas de cada conjunto\n",
        "train_real = {}\n",
        "val_real = {}\n",
        "test_real = {}\n",
        "\n",
        "for clase in clases:\n",
        "    imagenes = imagenes_reales[clase].copy()\n",
        "    random.shuffle(imagenes)\n",
        "\n",
        "    total = len(imagenes)\n",
        "    n_train = int(total * porc_train)\n",
        "    n_val = int(total * porc_val)\n",
        "    # El resto va a test, asegurando que se asignen todas las imágenes\n",
        "    n_test = total - n_train - n_val\n",
        "\n",
        "    train_real[clase] = imagenes[:n_train]\n",
        "    val_real[clase] = imagenes[n_train : n_train + n_val]\n",
        "    test_real[clase] = imagenes[n_train + n_val :]\n",
        "\n",
        "    print(\n",
        "        f\"{clase} - Total: {total}, Train: {len(train_real[clase])}, Val: {len(val_real[clase])}, Test: {len(test_real[clase])}\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Conjunto      Clase                                               Ruta\n",
            "0    train  Bacterial  ./data/covid-chest-xray\\Bacterial\\Bacterial-45...\n",
            "1    train  Bacterial  ./data/covid-chest-xray\\Bacterial\\Bacterial-4.jpg\n",
            "2    train  Bacterial  ./data/covid-chest-xray\\Bacterial\\Bacterial-40...\n",
            "3    train  Bacterial  ./data/covid-chest-xray\\Bacterial\\Bacterial-2.jpg\n",
            "4    train  Bacterial  ./data/covid-chest-xray\\Bacterial\\Bacterial-38...\n",
            "Conjunto  Clase    \n",
            "test      Bacterial      8\n",
            "          COVID-19      52\n",
            "          Normal        74\n",
            "train     Bacterial     33\n",
            "          COVID-19     239\n",
            "          Normal       340\n",
            "val       Bacterial      7\n",
            "          COVID-19      51\n",
            "          Normal        72\n",
            "dtype: int64\n",
            "Registro de imágenes guardado en: ./results/csv_files\\registro_imagenes_reales_multiclass.csv\n"
          ]
        }
      ],
      "source": [
        "def crear_registro(rutas_dict, conjunto_name):\n",
        "    registros = []\n",
        "    for clase, rutas in rutas_dict.items():\n",
        "        for ruta in rutas:\n",
        "            registros.append({\"Conjunto\": conjunto_name, \"Clase\": clase, \"Ruta\": ruta})\n",
        "    return pd.DataFrame(registros)\n",
        "\n",
        "\n",
        "df_train = crear_registro(train_real, \"train\")\n",
        "df_val = crear_registro(val_real, \"val\")\n",
        "df_test = crear_registro(test_real, \"test\")\n",
        "\n",
        "# Concatenar y mostrar el registro completo\n",
        "df_registro = pd.concat([df_train, df_val, df_test], ignore_index=True)\n",
        "print(df_registro.head())\n",
        "print(df_registro.groupby([\"Conjunto\", \"Clase\"]).size())\n",
        "\n",
        "# Guardar el registro de imágenes reales\n",
        "csv_registro_imagenes = os.path.join(csv_dir, \"registro_imagenes_reales_multiclass.csv\")\n",
        "df_registro.to_csv(csv_registro_imagenes, index=False)\n",
        "print(f\"Registro de imágenes guardado en: {csv_registro_imagenes}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Bacterial: 1000 imágenes aumentadas\n",
            "COVID-19: 1000 imágenes aumentadas\n",
            "Normal: 1000 imágenes aumentadas\n"
          ]
        }
      ],
      "source": [
        "# Diccionario para almacenar rutas de imágenes aumentadas por clase\n",
        "imagenes_aug = {}\n",
        "for clase in clases:\n",
        "    ruta_clase_aug = os.path.join(augmented_data_dir, clase)\n",
        "    imagenes_aug[clase] = obtener_rutas_imagenes(ruta_clase_aug)\n",
        "    print(f\"{clase}: {len(imagenes_aug[clase])} imágenes aumentadas\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Bacterial - Train final: 33 reales + 1000 aumentadas = 1033 imágenes\n",
            "COVID-19 - Train final: 239 reales + 1000 aumentadas = 1239 imágenes\n",
            "Normal - Train final: 340 reales + 1000 aumentadas = 1340 imágenes\n"
          ]
        }
      ],
      "source": [
        "# Diccionario para el conjunto de entrenamiento final (reales + aumentadas)\n",
        "train_final = {}\n",
        "\n",
        "for clase in clases:\n",
        "    reales = train_real[clase]\n",
        "    aumentadas = imagenes_aug.get(clase, [])\n",
        "    train_final[clase] = reales + aumentadas\n",
        "    print(\n",
        "        f\"{clase} - Train final: {len(reales)} reales + {len(aumentadas)} aumentadas = {len(train_final[clase])} imágenes\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Clase\n",
            "Bacterial    1033\n",
            "COVID-19     1239\n",
            "Normal       1340\n",
            "dtype: int64\n",
            "Registro del conjunto de entrenamiento final guardado en: ./results/csv_files\\registro_train_final_multiclass.csv\n"
          ]
        }
      ],
      "source": [
        "df_train_final = crear_registro(train_final, \"train_final\")\n",
        "\n",
        "# Mostrar resumen por clase\n",
        "print(df_train_final.groupby(\"Clase\").size())\n",
        "\n",
        "# Guardar el registro del conjunto de entrenamiento final (reales + aumentadas)\n",
        "csv_train_final = os.path.join(csv_dir, \"registro_train_final_multiclass.csv\")\n",
        "df_train_final.to_csv(csv_train_final, index=False)\n",
        "print(f\"Registro del conjunto de entrenamiento final guardado en: {csv_train_final}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "División de imágenes reales:\n",
            "         Conjunto  Bacterial  COVID-19  Normal\n",
            "0  Train (reales)         33       239     340\n",
            "1             Val          7        51      72\n",
            "2            Test          8        52      74\n",
            "\n",
            "Conjunto de entrenamiento final (reales + aumentadas):\n",
            "       Clase  Train_final\n",
            "0  Bacterial         1033\n",
            "1   COVID-19         1239\n",
            "2     Normal         1340\n"
          ]
        }
      ],
      "source": [
        "# Resumen de las cantidades en cada conjunto\n",
        "resumen = {\n",
        "    \"Conjunto\": [\"Train (reales)\", \"Val\", \"Test\"],\n",
        "    \"Bacterial\": [\n",
        "        len(train_real[\"Bacterial\"]),\n",
        "        len(val_real[\"Bacterial\"]),\n",
        "        len(test_real[\"Bacterial\"]),\n",
        "    ],\n",
        "    \"COVID-19\": [\n",
        "        len(train_real[\"COVID-19\"]),\n",
        "        len(val_real[\"COVID-19\"]),\n",
        "        len(test_real[\"COVID-19\"]),\n",
        "    ],\n",
        "    \"Normal\": [\n",
        "        len(train_real[\"Normal\"]),\n",
        "        len(val_real[\"Normal\"]),\n",
        "        len(test_real[\"Normal\"]),\n",
        "    ],\n",
        "}\n",
        "\n",
        "df_resumen = pd.DataFrame(resumen)\n",
        "print(\"División de imágenes reales:\")\n",
        "print(df_resumen)\n",
        "\n",
        "# Resumen de imágenes en el entrenamiento final (reales + aumentadas)\n",
        "resumen_train_final = {\n",
        "    \"Clase\": clases,  # [\"Bacterial\", \"COVID-19\", \"Normal\"]\n",
        "    \"Train_final\": [len(train_final[clase]) for clase in clases],\n",
        "}\n",
        "\n",
        "df_resumen_train_final = pd.DataFrame(resumen_train_final)\n",
        "print(\"\\nConjunto de entrenamiento final (reales + aumentadas):\")\n",
        "print(df_resumen_train_final)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 3612 validated image filenames belonging to 3 classes.\n",
            "Clases encontradas: {'Bacterial': 0, 'COVID-19': 1, 'Normal': 2}\n",
            "Found 134 validated image filenames belonging to 3 classes.\n",
            "Clases encontradas en test: {'Bacterial': 0, 'COVID-19': 1, 'Normal': 2}\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# Crear generador para el conjunto de entrenamiento (solo rescaling)\n",
        "train_datagen = ImageDataGenerator(rescale=1.0 / 255)\n",
        "\n",
        "# Crear el generador de datos a partir del DataFrame de entrenamiento (imágenes reales + aumentadas)\n",
        "train_generator = train_datagen.flow_from_dataframe(\n",
        "    dataframe=df_train_final,  # DataFrame con las imágenes de entrenamiento\n",
        "    x_col=\"Ruta\",  # Columna con las rutas de las imágenes\n",
        "    y_col=\"Clase\",  # Columna con las etiquetas de clase\n",
        "    target_size=(224, 224),  # Redimensionar las imágenes a 224x224\n",
        "    batch_size=32,\n",
        "    class_mode=\"categorical\",  # Para clasificación multiclase\n",
        "    shuffle=True,\n",
        ")\n",
        "\n",
        "print(\"Clases encontradas:\", train_generator.class_indices)\n",
        "\n",
        "# Crear generador para test (solo rescaling)\n",
        "test_datagen = ImageDataGenerator(rescale=1.0 / 255)\n",
        "\n",
        "# Filtrar el DataFrame para obtener solo las imágenes del conjunto \"test\"\n",
        "df_test = df_registro[df_registro[\"Conjunto\"] == \"test\"]\n",
        "\n",
        "# Crear el generador de datos para el conjunto de test\n",
        "test_generator = test_datagen.flow_from_dataframe(\n",
        "    dataframe=df_test,  # DataFrame con las imágenes reales de test\n",
        "    x_col=\"Ruta\",  # Columna que contiene las rutas de las imágenes\n",
        "    y_col=\"Clase\",  # Columna con las etiquetas de clase\n",
        "    target_size=(224, 224),  # Redimensiona las imágenes a 224x224\n",
        "    batch_size=32,\n",
        "    class_mode=\"categorical\",  # Para clasificación multiclase\n",
        "    shuffle=False,  # No se baraja para mantener el orden de test\n",
        ")\n",
        "\n",
        "print(\"Clases encontradas en test:\", test_generator.class_indices)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jc3EsCw8YUVZ",
        "outputId": "62fbb4bf-e399-42a1-db55-a5bfaeebc92b"
      },
      "outputs": [],
      "source": [
        "# Importar librerías necesarias\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import (\n",
        "    Conv2D,\n",
        "    MaxPooling2D,\n",
        "    Dropout,\n",
        "    Flatten,\n",
        "    Dense,\n",
        "    BatchNormalization,\n",
        "    Activation,\n",
        ")\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from keras_tuner.tuners import RandomSearch\n",
        "import json"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4IbHdRdMYWk9"
      },
      "source": [
        "# Definición del modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# Definir la función que construye el modelo de CNN, con hiperparámetros ajustables.\n",
        "# =============================================================================\n",
        "def build_cnn(hp):\n",
        "    model = Sequential(\n",
        "        [\n",
        "            # Primera capa convolucional: se parametriza el número de filtros (entre 32 y 64, de 16 en 16)\n",
        "            Conv2D(\n",
        "                hp.Int(\"filters_1\", 32, 64, step=16),\n",
        "                (3, 3),\n",
        "                activation=None,\n",
        "                input_shape=(224, 224, 3),\n",
        "            ),\n",
        "            BatchNormalization(),\n",
        "            Activation(\"relu\"),\n",
        "            MaxPooling2D(2, 2),\n",
        "            Dropout(hp.Float(\"dropout_1\", 0.2, 0.5, step=0.1)),\n",
        "            \n",
        "            # Segunda capa convolucional\n",
        "            Conv2D(hp.Int(\"filters_2\", 64, 128, step=32), (3, 3), activation=None),\n",
        "            BatchNormalization(),\n",
        "            Activation(\"relu\"),\n",
        "            MaxPooling2D(2, 2),\n",
        "            Dropout(hp.Float(\"dropout_2\", 0.2, 0.5, step=0.1)),\n",
        "            \n",
        "            # Tercera capa convolucional\n",
        "            Conv2D(hp.Int(\"filters_3\", 128, 256, step=64), (3, 3), activation=None),\n",
        "            BatchNormalization(),\n",
        "            Activation(\"relu\"),\n",
        "            MaxPooling2D(2, 2),\n",
        "            Dropout(hp.Float(\"dropout_3\", 0.3, 0.6, step=0.1)),\n",
        "            \n",
        "            # Capa de aplanamiento para pasar a las capas densas\n",
        "            Flatten(),\n",
        "            \n",
        "            # Capa densa con unidades parametrizables (entre 64 y 256, de 64 en 64)\n",
        "            Dense(hp.Int(\"dense_units\", 64, 256, step=64), activation=None),\n",
        "            BatchNormalization(),\n",
        "            Activation(\"relu\"),\n",
        "            Dropout(hp.Float(\"dropout_dense\", 0.3, 0.6, step=0.1)),\n",
        "            \n",
        "            # Capa de salida: 3 neuronas para 3 clases, con activación softmax\n",
        "            Dense(3, activation=\"softmax\"),\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    # Definir el optimizador: Adam con tasa de aprendizaje seleccionada entre tres opciones.\n",
        "    optimizer = Adam(learning_rate=hp.Choice(\"learning_rate\", [1e-4, 5e-4, 1e-3]))\n",
        "\n",
        "    # Compilar el modelo usando entropía cruzada categórica (para clasificación multiclase)\n",
        "    # Se añaden varias métricas para evaluar: accuracy, precision, recall y AUC.\n",
        "    model.compile(\n",
        "        loss=\"categorical_crossentropy\",\n",
        "        optimizer=optimizer,\n",
        "        metrics=[\n",
        "            \"accuracy\", \n",
        "            tf.keras.metrics.Precision(), \n",
        "            tf.keras.metrics.Recall(), \n",
        "            tf.keras.metrics.AUC()\n",
        "        ],\n",
        "    )\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-yj45LgkYb72"
      },
      "source": [
        "# Carga de datos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VY3NrE6fYpyA"
      },
      "source": [
        "# Entrenamiento del modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TensorFlow versión: 2.10.0\n",
            "GPUs disponibles: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
            "Tensor de prueba:\n",
            " tf.Tensor(\n",
            "[[1. 2.]\n",
            " [3. 4.]], shape=(2, 2), dtype=float32)\n"
          ]
        }
      ],
      "source": [
        "# PRUEBA FUNCIONAMIENTO TENSORFLOW CON GPU\n",
        "\n",
        "# Mostrar la versión de TensorFlow\n",
        "print(\"TensorFlow versión:\", tf.__version__)\n",
        "\n",
        "# Listar GPUs disponibles\n",
        "print(\"GPUs disponibles:\", tf.config.list_physical_devices('GPU'))\n",
        "\n",
        "# Crear un tensor simple para probar\n",
        "tensor = tf.constant([[1.0, 2.0], [3.0, 4.0]])\n",
        "print(\"Tensor de prueba:\\n\", tensor)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Trial 10 Complete [00h 02m 54s]\n",
            "val_accuracy: 0.8432835936546326\n",
            "\n",
            "Best val_accuracy So Far: 0.9850746393203735\n",
            "Total elapsed time: 00h 31m 49s\n"
          ]
        }
      ],
      "source": [
        "# =============================================================================\n",
        "# Configurar el Random Search para buscar los mejores hiperparámetros\n",
        "# =============================================================================\n",
        "tuner = RandomSearch(\n",
        "    build_cnn,  # Función que construye el modelo\n",
        "    objective=\"val_accuracy\",  # Métrica a optimizar (precisión en validación)\n",
        "    max_trials=10,  # Número máximo de combinaciones a probar\n",
        "    executions_per_trial=1,  # Número de ejecuciones por cada combinación\n",
        "    directory=\"cnn_chest-xray_problem2\",  # Directorio para guardar resultados del tuner\n",
        "    project_name=\"cnn_chest-xray_problem2\",  # Nombre del proyecto\n",
        ")\n",
        "\n",
        "# =============================================================================\n",
        "# Definir Early Stopping para detener el entrenamiento si no mejora la precisión de validación\n",
        "# =============================================================================\n",
        "early_stopping = EarlyStopping(\n",
        "    monitor=\"val_accuracy\", patience=5, restore_best_weights=True\n",
        ")\n",
        "\n",
        "# =============================================================================\n",
        "# Realizar la búsqueda de hiperparámetros usando los generadores de entrenamiento y validación.\n",
        "# Se asume que 'train_generator' y 'test_generator' están definidos previamente.\n",
        "# =============================================================================\n",
        "tuner.search(\n",
        "    train_generator,\n",
        "    validation_data=test_generator,  # Aquí test_generator actúa como conjunto de validación\n",
        "    epochs=25,\n",
        "    callbacks=[early_stopping],\n",
        "    verbose=1,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Results summary\n",
            "Results in cnn_chest-xray_problem2\\cnn_chest-xray_problem2\n",
            "Showing 10 best trials\n",
            "Objective(name=\"val_accuracy\", direction=\"max\")\n",
            "\n",
            "Trial 03 summary\n",
            "Hyperparameters:\n",
            "filters_1: 48\n",
            "dropout_1: 0.4\n",
            "filters_2: 128\n",
            "dropout_2: 0.30000000000000004\n",
            "filters_3: 128\n",
            "dropout_3: 0.4\n",
            "dense_units: 256\n",
            "dropout_dense: 0.3\n",
            "learning_rate: 0.001\n",
            "Score: 0.9850746393203735\n",
            "\n",
            "Trial 00 summary\n",
            "Hyperparameters:\n",
            "filters_1: 48\n",
            "dropout_1: 0.30000000000000004\n",
            "filters_2: 128\n",
            "dropout_2: 0.30000000000000004\n",
            "filters_3: 256\n",
            "dropout_3: 0.5\n",
            "dense_units: 128\n",
            "dropout_dense: 0.4\n",
            "learning_rate: 0.0005\n",
            "Score: 0.9776119589805603\n",
            "\n",
            "Trial 01 summary\n",
            "Hyperparameters:\n",
            "filters_1: 48\n",
            "dropout_1: 0.2\n",
            "filters_2: 64\n",
            "dropout_2: 0.30000000000000004\n",
            "filters_3: 256\n",
            "dropout_3: 0.3\n",
            "dense_units: 192\n",
            "dropout_dense: 0.5\n",
            "learning_rate: 0.0005\n",
            "Score: 0.9701492786407471\n",
            "\n",
            "Trial 04 summary\n",
            "Hyperparameters:\n",
            "filters_1: 64\n",
            "dropout_1: 0.30000000000000004\n",
            "filters_2: 64\n",
            "dropout_2: 0.2\n",
            "filters_3: 192\n",
            "dropout_3: 0.3\n",
            "dense_units: 192\n",
            "dropout_dense: 0.5\n",
            "learning_rate: 0.001\n",
            "Score: 0.9626865386962891\n",
            "\n",
            "Trial 05 summary\n",
            "Hyperparameters:\n",
            "filters_1: 48\n",
            "dropout_1: 0.30000000000000004\n",
            "filters_2: 64\n",
            "dropout_2: 0.30000000000000004\n",
            "filters_3: 192\n",
            "dropout_3: 0.3\n",
            "dense_units: 128\n",
            "dropout_dense: 0.3\n",
            "learning_rate: 0.0001\n",
            "Score: 0.9626865386962891\n",
            "\n",
            "Trial 06 summary\n",
            "Hyperparameters:\n",
            "filters_1: 32\n",
            "dropout_1: 0.2\n",
            "filters_2: 128\n",
            "dropout_2: 0.4\n",
            "filters_3: 256\n",
            "dropout_3: 0.3\n",
            "dense_units: 128\n",
            "dropout_dense: 0.3\n",
            "learning_rate: 0.0005\n",
            "Score: 0.9626865386962891\n",
            "\n",
            "Trial 07 summary\n",
            "Hyperparameters:\n",
            "filters_1: 64\n",
            "dropout_1: 0.4\n",
            "filters_2: 64\n",
            "dropout_2: 0.2\n",
            "filters_3: 256\n",
            "dropout_3: 0.4\n",
            "dense_units: 192\n",
            "dropout_dense: 0.5\n",
            "learning_rate: 0.0005\n",
            "Score: 0.9626865386962891\n",
            "\n",
            "Trial 08 summary\n",
            "Hyperparameters:\n",
            "filters_1: 48\n",
            "dropout_1: 0.4\n",
            "filters_2: 64\n",
            "dropout_2: 0.4\n",
            "filters_3: 128\n",
            "dropout_3: 0.3\n",
            "dense_units: 192\n",
            "dropout_dense: 0.3\n",
            "learning_rate: 0.0005\n",
            "Score: 0.9626865386962891\n",
            "\n",
            "Trial 02 summary\n",
            "Hyperparameters:\n",
            "filters_1: 48\n",
            "dropout_1: 0.4\n",
            "filters_2: 96\n",
            "dropout_2: 0.4\n",
            "filters_3: 256\n",
            "dropout_3: 0.5\n",
            "dense_units: 128\n",
            "dropout_dense: 0.5\n",
            "learning_rate: 0.001\n",
            "Score: 0.9402984976768494\n",
            "\n",
            "Trial 09 summary\n",
            "Hyperparameters:\n",
            "filters_1: 64\n",
            "dropout_1: 0.2\n",
            "filters_2: 96\n",
            "dropout_2: 0.4\n",
            "filters_3: 256\n",
            "dropout_3: 0.3\n",
            "dense_units: 256\n",
            "dropout_dense: 0.5\n",
            "learning_rate: 0.0001\n",
            "Score: 0.8432835936546326\n"
          ]
        }
      ],
      "source": [
        "# Mostrar resumen de la búsqueda de hiperparámetros\n",
        "tuner.results_summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OaitSY2qdag7",
        "outputId": "de503464-d17f-42a8-9323-319d5595f412"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mejor modelo encontrado con filtros: 48, 128, 128, learning rate: 0.001\n"
          ]
        }
      ],
      "source": [
        "# =============================================================================\n",
        "# Obtener el mejor modelo y sus hiperparámetros encontrados\n",
        "# =============================================================================\n",
        "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
        "best_model = tuner.get_best_models(num_models=1)[0]\n",
        "print(\n",
        "    f\"Mejor modelo encontrado con filtros: {best_hps.get('filters_1')}, {best_hps.get('filters_2')}, {best_hps.get('filters_3')}, learning rate: {best_hps.get('learning_rate')}\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "fQut8wEesIm7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/25\n",
            "113/113 [==============================] - 13s 109ms/step - loss: 0.0257 - accuracy: 0.9925 - precision: 0.9925 - recall: 0.9925 - auc: 0.9999 - val_loss: 0.2510 - val_accuracy: 0.9403 - val_precision: 0.9403 - val_recall: 0.9403 - val_auc: 0.9765\n",
            "Epoch 2/25\n",
            "113/113 [==============================] - 12s 108ms/step - loss: 0.0216 - accuracy: 0.9945 - precision: 0.9945 - recall: 0.9945 - auc: 0.9999 - val_loss: 0.1014 - val_accuracy: 0.9701 - val_precision: 0.9699 - val_recall: 0.9627 - val_auc: 0.9974\n",
            "Epoch 3/25\n",
            "113/113 [==============================] - 12s 107ms/step - loss: 0.0280 - accuracy: 0.9909 - precision: 0.9909 - recall: 0.9909 - auc: 0.9998 - val_loss: 0.3145 - val_accuracy: 0.8881 - val_precision: 0.8881 - val_recall: 0.8881 - val_auc: 0.9729\n",
            "Epoch 4/25\n",
            "113/113 [==============================] - 12s 107ms/step - loss: 0.0275 - accuracy: 0.9920 - precision: 0.9922 - recall: 0.9920 - auc: 0.9998 - val_loss: 0.2678 - val_accuracy: 0.9328 - val_precision: 0.9328 - val_recall: 0.9328 - val_auc: 0.9772\n",
            "Epoch 5/25\n",
            "113/113 [==============================] - 12s 109ms/step - loss: 0.0359 - accuracy: 0.9884 - precision: 0.9892 - recall: 0.9878 - auc: 0.9995 - val_loss: 0.7091 - val_accuracy: 0.9328 - val_precision: 0.9328 - val_recall: 0.9328 - val_auc: 0.9594\n",
            "Epoch 6/25\n",
            "113/113 [==============================] - 13s 116ms/step - loss: 0.0179 - accuracy: 0.9947 - precision: 0.9947 - recall: 0.9947 - auc: 1.0000 - val_loss: 0.1113 - val_accuracy: 0.9627 - val_precision: 0.9627 - val_recall: 0.9627 - val_auc: 0.9971\n",
            "Epoch 7/25\n",
            "113/113 [==============================] - 13s 111ms/step - loss: 0.0180 - accuracy: 0.9947 - precision: 0.9947 - recall: 0.9947 - auc: 0.9999 - val_loss: 0.1422 - val_accuracy: 0.9478 - val_precision: 0.9478 - val_recall: 0.9478 - val_auc: 0.9922\n",
            "Epoch 8/25\n",
            "113/113 [==============================] - 12s 105ms/step - loss: 0.0263 - accuracy: 0.9917 - precision: 0.9917 - recall: 0.9914 - auc: 0.9998 - val_loss: 0.1640 - val_accuracy: 0.9627 - val_precision: 0.9627 - val_recall: 0.9627 - val_auc: 0.9915\n",
            "Epoch 9/25\n",
            "113/113 [==============================] - 12s 103ms/step - loss: 0.0290 - accuracy: 0.9917 - precision: 0.9920 - recall: 0.9917 - auc: 0.9998 - val_loss: 2.6775 - val_accuracy: 0.5821 - val_precision: 0.5821 - val_recall: 0.5821 - val_auc: 0.8008\n",
            "Epoch 10/25\n",
            "113/113 [==============================] - 12s 102ms/step - loss: 0.0219 - accuracy: 0.9928 - precision: 0.9928 - recall: 0.9928 - auc: 0.9999 - val_loss: 0.1616 - val_accuracy: 0.9701 - val_precision: 0.9701 - val_recall: 0.9701 - val_auc: 0.9859\n",
            "Epoch 11/25\n",
            "113/113 [==============================] - 12s 102ms/step - loss: 0.0263 - accuracy: 0.9914 - precision: 0.9920 - recall: 0.9909 - auc: 0.9998 - val_loss: 0.4563 - val_accuracy: 0.9403 - val_precision: 0.9403 - val_recall: 0.9403 - val_auc: 0.9660\n",
            "Epoch 12/25\n",
            "113/113 [==============================] - 12s 103ms/step - loss: 0.0233 - accuracy: 0.9936 - precision: 0.9936 - recall: 0.9936 - auc: 0.9998 - val_loss: 0.6131 - val_accuracy: 0.9328 - val_precision: 0.9328 - val_recall: 0.9328 - val_auc: 0.9614\n",
            "Epoch 13/25\n",
            "113/113 [==============================] - 11s 101ms/step - loss: 0.0215 - accuracy: 0.9936 - precision: 0.9939 - recall: 0.9934 - auc: 0.9999 - val_loss: 0.1211 - val_accuracy: 0.9627 - val_precision: 0.9627 - val_recall: 0.9627 - val_auc: 0.9929\n",
            "Epoch 14/25\n",
            "113/113 [==============================] - 12s 101ms/step - loss: 0.0226 - accuracy: 0.9936 - precision: 0.9939 - recall: 0.9936 - auc: 0.9998 - val_loss: 0.0941 - val_accuracy: 0.9701 - val_precision: 0.9701 - val_recall: 0.9701 - val_auc: 0.9981\n",
            "Epoch 15/25\n",
            "113/113 [==============================] - 11s 101ms/step - loss: 0.0230 - accuracy: 0.9925 - precision: 0.9931 - recall: 0.9922 - auc: 0.9999 - val_loss: 1.0430 - val_accuracy: 0.6866 - val_precision: 0.6894 - val_recall: 0.6791 - val_auc: 0.8739\n",
            "Epoch 16/25\n",
            "113/113 [==============================] - 11s 100ms/step - loss: 0.0202 - accuracy: 0.9936 - precision: 0.9939 - recall: 0.9936 - auc: 0.9999 - val_loss: 4.7529 - val_accuracy: 0.4552 - val_precision: 0.4552 - val_recall: 0.4552 - val_auc: 0.6719\n",
            "Epoch 17/25\n",
            "113/113 [==============================] - 12s 101ms/step - loss: 0.0196 - accuracy: 0.9934 - precision: 0.9934 - recall: 0.9934 - auc: 0.9999 - val_loss: 2.6532 - val_accuracy: 0.5149 - val_precision: 0.5149 - val_recall: 0.5149 - val_auc: 0.7536\n",
            "Epoch 18/25\n",
            "113/113 [==============================] - 12s 102ms/step - loss: 0.0368 - accuracy: 0.9892 - precision: 0.9895 - recall: 0.9889 - auc: 0.9991 - val_loss: 0.1183 - val_accuracy: 0.9627 - val_precision: 0.9627 - val_recall: 0.9627 - val_auc: 0.9978\n",
            "Epoch 19/25\n",
            "113/113 [==============================] - 12s 102ms/step - loss: 0.0320 - accuracy: 0.9898 - precision: 0.9898 - recall: 0.9898 - auc: 0.9993 - val_loss: 3.2214 - val_accuracy: 0.5672 - val_precision: 0.5672 - val_recall: 0.5672 - val_auc: 0.7803\n",
            "Epoch 20/25\n",
            "113/113 [==============================] - 12s 102ms/step - loss: 0.0226 - accuracy: 0.9925 - precision: 0.9928 - recall: 0.9925 - auc: 0.9999 - val_loss: 0.5367 - val_accuracy: 0.8358 - val_precision: 0.8358 - val_recall: 0.8358 - val_auc: 0.9421\n",
            "Epoch 21/25\n",
            "113/113 [==============================] - 12s 101ms/step - loss: 0.0139 - accuracy: 0.9970 - precision: 0.9970 - recall: 0.9967 - auc: 1.0000 - val_loss: 0.1122 - val_accuracy: 0.9776 - val_precision: 0.9776 - val_recall: 0.9776 - val_auc: 0.9921\n",
            "Epoch 22/25\n",
            "113/113 [==============================] - 12s 102ms/step - loss: 0.0102 - accuracy: 0.9975 - precision: 0.9975 - recall: 0.9975 - auc: 1.0000 - val_loss: 0.0900 - val_accuracy: 0.9627 - val_precision: 0.9627 - val_recall: 0.9627 - val_auc: 0.9990\n",
            "Epoch 23/25\n",
            "113/113 [==============================] - 12s 103ms/step - loss: 0.0112 - accuracy: 0.9967 - precision: 0.9967 - recall: 0.9964 - auc: 1.0000 - val_loss: 0.6130 - val_accuracy: 0.8955 - val_precision: 0.8955 - val_recall: 0.8955 - val_auc: 0.9559\n",
            "Epoch 24/25\n",
            "113/113 [==============================] - 12s 102ms/step - loss: 0.0077 - accuracy: 0.9989 - precision: 0.9989 - recall: 0.9989 - auc: 1.0000 - val_loss: 0.2715 - val_accuracy: 0.9478 - val_precision: 0.9478 - val_recall: 0.9478 - val_auc: 0.9821\n",
            "Epoch 25/25\n",
            "113/113 [==============================] - 11s 101ms/step - loss: 0.0055 - accuracy: 0.9994 - precision: 0.9994 - recall: 0.9994 - auc: 1.0000 - val_loss: 0.1038 - val_accuracy: 0.9776 - val_precision: 0.9776 - val_recall: 0.9776 - val_auc: 0.9878\n"
          ]
        }
      ],
      "source": [
        "# =============================================================================\n",
        "# Entrenar el mejor modelo utilizando el conjunto de entrenamiento y validación.\n",
        "# Se extrae el historial de entrenamiento para su análisis posterior.\n",
        "# =============================================================================\n",
        "history = best_model.fit(train_generator, validation_data=test_generator, epochs=25)\n",
        "\n",
        "# =============================================================================\n",
        "# Convertir el historial de entrenamiento a un DataFrame para analizar las métricas.\n",
        "# =============================================================================\n",
        "df_metrics = pd.DataFrame(history.history)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Métricas guardadas en: ./results/csv_files\\training_metrics_multiclass.csv\n"
          ]
        }
      ],
      "source": [
        "# =============================================================================\n",
        "# Guardar el DataFrame con las métricas de entrenamiento en un archivo CSV,\n",
        "# usando una estructura de carpetas organizada.\n",
        "# =============================================================================\n",
        "csv_dir = \"./results/csv_files\"\n",
        "os.makedirs(csv_dir, exist_ok=True)\n",
        "\n",
        "csv_metrics_path = os.path.join(csv_dir, \"training_metrics_multiclass.csv\")\n",
        "df_metrics.to_csv(csv_metrics_path, index=False)\n",
        "print(\"Métricas guardadas en:\", csv_metrics_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hb5IIvzoY7oM",
        "outputId": "4c5927e5-2c9e-46de-aafb-955511945f5b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mejor multiclass modelo guardado en: ./results/multiclass\\models\\best_multiclass_model.h5\n",
            "Pesos del multiclass modelo guardados en: ./results/multiclass\\weights\\best_multiclass.weights.h5\n",
            "Hiperparámetros del multiclass modelo guardados en: ./results/multiclass\\hyperparameters\\best_multiclass_hyperparameters.json\n"
          ]
        }
      ],
      "source": [
        "def save_model_structure(best_model, best_hps, model_type):\n",
        "    \"\"\"\n",
        "    Guarda el modelo completo, sus pesos y los hiperparámetros en una estructura organizada.\n",
        "\n",
        "    Args:\n",
        "        best_model: Modelo de Keras a guardar.\n",
        "        best_hps: Objeto de hiperparámetros (por ejemplo, de Keras Tuner).\n",
        "        model_type: Cadena identificadora (\"multiclass\" o \"binary\") para crear subcarpetas específicas.\n",
        "    \"\"\"\n",
        "    # Definir la carpeta base para el tipo de modelo (multiclass o binary)\n",
        "    base_dir = f\"./results/{model_type}\"\n",
        "\n",
        "    # Crear subcarpetas para modelos, pesos e hiperparámetros\n",
        "    models_dir = os.path.join(base_dir, \"models\")\n",
        "    weights_dir = os.path.join(base_dir, \"weights\")\n",
        "    hyperparams_dir = os.path.join(base_dir, \"hyperparameters\")\n",
        "\n",
        "    os.makedirs(models_dir, exist_ok=True)\n",
        "    os.makedirs(weights_dir, exist_ok=True)\n",
        "    os.makedirs(hyperparams_dir, exist_ok=True)\n",
        "\n",
        "    # Guardar el modelo completo en la carpeta 'models'\n",
        "    model_path = os.path.join(models_dir, f\"best_{model_type}_model.h5\")\n",
        "    best_model.save(model_path)\n",
        "    print(f\"Mejor {model_type} modelo guardado en:\", model_path)\n",
        "\n",
        "    # Guardar solo los pesos del modelo en la carpeta 'weights'\n",
        "    weights_path = os.path.join(weights_dir, f\"best_{model_type}.weights.h5\")\n",
        "    best_model.save_weights(weights_path)\n",
        "    print(f\"Pesos del {model_type} modelo guardados en:\", weights_path)\n",
        "\n",
        "    # Guardar los hiperparámetros del modelo en formato JSON en la carpeta 'hyperparameters'\n",
        "    hyperparams_path = os.path.join(\n",
        "        hyperparams_dir, f\"best_{model_type}_hyperparameters.json\"\n",
        "    )\n",
        "    with open(hyperparams_path, \"w\") as json_file:\n",
        "        json.dump(best_hps.values, json_file, indent=4)\n",
        "    print(f\"Hiperparámetros del {model_type} modelo guardados en:\", hyperparams_path)\n",
        "\n",
        "\n",
        "# Ejemplo de uso para el modelo multiclase:\n",
        "save_model_structure(best_model, best_hps, \"multiclass\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
